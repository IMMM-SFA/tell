![build](https://github.com/IMMM-SFA/tell/workflows/build/badge.svg) [![codecov](https://codecov.io/gh/IMMM-SFA/tell/branch/package/graph/badge.svg?token=URP1KWRI6U)](https://codecov.io/gh/IMMM-SFA/tell)

# TELL
The Total ELectricity Load (TELL) model provides a framework that integrates aspects of both short- and long-term predictions of electricity demand in a coherent and scalable way. TELL takes as input gridded hourly time-series of meteorology and uses the temporal variations in weather to predict hourly profiles of total electricity demand for every county in the lower 48 United States using a multilayer perceptron (MLP) approach. Hourly predictions from TELL are then scaled to match the annual state-level total electricity loads predicted by the U.S. version of the Global Change Analysis Model (GCAM-USA). GCAM-USA is designed to capture the long-term co-evolution of the human-Earth system. Using this unique approach allows TELL to reflect both changes in the shape of the load profile due to variations in weather and climate and the long-term evolution of energy demand due to changes in population, technology, and economics. TELL is unique from other probabilistic load forecasting models in that it features an explicit spatial component that allows us to relate predicted loads to where they would occur spatially within a grid operations model.


###Data Download

**1.0 Automatically download a data package containing the EIA-861, EIA-930, and census population datasets**

```buildoutcfg
TODO
```

###Data Processing

**1.1 Use the EIA-861 dataset to create a mapping between BAs and counties**

```buildoutcfg
import os
import tell


if __name__ == '__main__':

    # set the target year
    target_year = 2015

    # get the input directory as it currently exists within this repository
    input_dir = os.path.join(os.path.dirname(__file__), 'inputs')

    # get the path to the EIA raw data for the target year
    eia_data_dir = os.path.join(input_dir, 'EIA_861', 'Raw_Data', str(target_year))

    # directory containing the outputs
    output_dir = os.path.join(os.path.dirname(__file__), 'outputs')

    # paths to files
    fips_file = os.path.join(input_dir, 'state_and_county_fips_codes.xlsx')
    service_area_file = os.path.join(eia_data_dir, f'Service_Territory_{target_year}.xlsx')
    sales_ult_file = os.path.join(eia_data_dir, f'Sales_Ult_Cust_{target_year}.xlsx')
    bal_auth_file = os.path.join(eia_data_dir, f'Balancing_Authority_{target_year}.xlsx')

    # prepare data
    tell.process_data(target_year, fips_file, service_area_file, sales_ult_file, bal_auth_file, output_dir)


    output = os.path.join(output_dir, f'fips_service_match_{target_year}.csv')
    output = pd.read_csv(output)
    output2 = output.drop_duplicates()
    output_file = os.path.join(output_dir, f'fips_service_match_rm{target_year}.csv')
    output2.to_csv(output_file, sep=',', index=False)
```

**1.2 Convert the EIA-930 BA demand dataset from Excel files into CSV files**

```buildoutcfg
# Set the data input and output directories:
EIA_930_input_dir = 
EIA_930_output_dir = 'os.path.join(os.path.dirname(__file__), 'outputs')

# Process the hourly load data
tell.process_EIA_930(EIA_930_input_dir, EIA_930_output_dir)
```

**1.3 Average WRF meteorology within each BA’s territory**

```buildoutcfg
TODO
```

**1.4 Compute time-series of total population within each BA’s territory**
```buildoutcfg
# Set the data input and output directories:
population_input_dir = '//connie-1/im3fs/tell/inputs'
mapping_input_dir = '//connie-1/im3fs/tell/inputs/Utility_Mapping/CSV_Files'
pop_output_dir = 'C:/Users/mcgr323/projects/tell/BA_hourly_inputs/BA_population'

# Set some processing flags:
start_year = 2015;  # Starting year of time series
end_year = 2019;  # Ending year of time series

tell.ba_pop_sum(mapping_input_dir, population_input_dir, start_year, end_year)
```

**1.5 Merge the hourly-time series of meteorology from 1.3, population from 1.4, and demand for each BA from 1.2 into a single CSV file that can be used as input to 2.2**
```buildoutcfg
TODO
```

###Run the MLP Simulation

**2.1  Clean the data generated by 1.5 to remove outliers**
```buildoutcfg
TODO
```

**2.2 Build the MLP models that relate historical variations in meteorology and population to the historical time-series of hourly demand in each BA (@Aowabin Rahman). The code should have options for the user to specify the training and evaluation periods.**
```buildoutcfg
#set the input and output directories 
data_dir = 'C:/Users/mcgr323/OneDrive - PNNL/Desktop/WRF_CSV_Files/CSV_Files'
output_dir = 'C:/Users/mcgr323/OneDrive - PNNL/Desktop/WRF_predict_output'

#set parameters 
batch_run = True
target_ba_list = None
generate_plots = True

#initally time
t0 = time.time()

#run the model 
df = tell.predict(data_dir=data_dir,
                  out_dir=output_dir,
                  batch_run=batch_run,
                  target_ba_list=target_ba_list,
                  generate_plots=generate_plots)
```

**2.3  Calculate performance metrics and generate quick look plots to be used to evaluate the model**
```buildoutcfg
TODO
```

###Model Forward Execution 

```buildoutcfg
import tell
import datetime
import glob

# Set a time variable to benchmark the run time:
begin_time = datetime.datetime.now()

# Set the year and GCAM-USA scenario to process:
year_to_process = '2020'
gcam_usa_scenario = 'scenario_name'

# Set the data input and output directories:
mlp_input_dir = '/Users/burl878/OneDrive - PNNL/Documents/IMMM/Data/TELL_Input_Data/outputs/MLP_Model_Output/' + year_to_process + '/'
ba_geolocation_input_dir = '/Users/burl878/OneDrive - PNNL/Documents/IMMM/Data/TELL_Input_Data/inputs/Utility_Mapping/CSV_Files/'
population_input_dir = '/Users/burl878/OneDrive - PNNL/Documents/IMMM/Data/TELL_Input_Data/inputs/'
gcam_usa_input_dir = '/Users/burl878/OneDrive - PNNL/Documents/IMMM/Data/TELL_Input_Data/forward_execution/GCAM_USA_Forcing/Raw_Data/' + gcam_usa_scenario + '/'
data_output_dir = '/Users/burl878/OneDrive - PNNL/Documents/IMMM/Data/TELL_Input_Data/outputs/' + year_to_process + '/'

# Load in the accompanying GCAM-USA output file and subset to the "year_to_process":
gcam_usa_df = tell.extract_gcam_usa_loads((gcam_usa_input_dir + 'gcamDataTable_aggParam.csv'))
gcam_usa_df = tell.gcam_usa_df[gcam_usa_df['Year'] == int(year_to_process)]

# Load in the most recent (e.g., 2019) BA service territory map and simplify the dataframe:
ba_mapping = pd.read_csv((ba_geolocation_input_dir + 'ba_service_territory_2019.csv'), index_col=None, header=0)

# Load in the population data and simplify the dataframe:
population = pd.read_csv(population_input_dir + '/county_populations_2000_to_2019.csv')
population = population[{'county_FIPS', 'pop_2019'}].copy(deep=False)
population.rename(columns={"county_FIPS":"County_FIPS",
                           "pop_2019":"Population"}, inplace=True)

# Merge the ba_mapping and population dataframes together. Compute the fraction of the
# total population in each BA that lives in a given county:
mapping_df = ba_mapping.merge(population, on=['County_FIPS'])
mapping_df = mapping_df.sort_values("BA_Number")
mapping_df['BA_Population_Sum'] = mapping_df.groupby('BA_Code')['Population'].transform('sum')
mapping_df['BA_Population_Fraction'] = mapping_df['Population'] / mapping_df['BA_Population_Sum']
mapping_df = mapping_df.dropna()
del population, ba_mapping

# Create a list of all of the MLP output files in the "data_input_dir" and aggregate the files
# in that list using the "aggregate_mlp_output_files" function:
mlp_output_df = tell.aggregate_mlp_output_files(sorted(glob.glob(os.path.join(mlp_input_dir + '*_mlp_predictions.csv'))))

# Merge the "mapping_df" with "mlp_output_df":
joint_mlp_df = pd.merge(mlp_output_df, mapping_df, on='BA_Code')

# Scale the BA loads in each county by the fraction of the BA's total population that lives there:
joint_mlp_df['County_BA_Load_MWh'] = joint_mlp_df['Total_BA_Load_MWh'].mul(joint_mlp_df['BA_Population_Fraction'])

# Sum the county-level hourly loads into annual state-level total loads and convert that value from MWh to TWh:
joint_mlp_df['TELL_State_Annual_Load_TWh'] = (joint_mlp_df.groupby('State_FIPS')['County_BA_Load_MWh'].transform('sum')) / 1000000

# Add a column with the state-level annual total loads from GCAM-USA:
joint_mlp_df = pd.merge(joint_mlp_df, gcam_usa_df[['State_FIPS', 'GCAM_USA_State_Annual_Load_TWh']], on='State_FIPS', how='left')

# Compute the state-level scaling factors that force TELL annual loads to match GCAM-USA annual loads:
joint_mlp_df['State_Scaling_Factor'] = joint_mlp_df['GCAM_USA_State_Annual_Load_TWh'].div(joint_mlp_df['TELL_State_Annual_Load_TWh'])

# Apply those scaling factors to the "County_BA_Load_MWh" value:
joint_mlp_df['County_BA_Load_MWh_Scaled'] = joint_mlp_df['County_BA_Load_MWh'].mul(joint_mlp_df['State_Scaling_Factor'])

# Output the data using the output functions:
tell.output_tell_summary_data(joint_mlp_df, data_output_dir)
tell.output_tell_ba_data(joint_mlp_df, data_output_dir)
tell.output_tell_state_data(joint_mlp_df, data_output_dir)
tell.output_tell_county_data(joint_mlp_df, data_output_dir)

# Output the elapsed time in order to benchmark the run time:
print('Elapsed time = ', datetime.datetime.now() - begin_time)

```

###Model visualizations 
