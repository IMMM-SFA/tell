{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e071b84",
   "metadata": {},
   "source": [
    "# Welcome to the TELL Quickstarter! \n",
    "\n",
    "### **`tell` is an open-source Python package for predicting future electricty load in the United States.**\n",
    "\n",
    "## A little about `tell`\n",
    "\n",
    "The Total ELectricity Load (TELL) model predicts the short- and long-term evoluation of hourly electricity demand in response to future climate and population changes. The purpose of `tell` is to generate end-of-century hourly profiles of electricity demand across the entire United States (U.S.) at a spatial resolution adequate for input to a unit commitment/economic dispatch (UC/ED) model while also maintaining consistency with the long-term growth and evolution of annual state-level electricity demand projected by an economically driven human-Earth system model. `tell` takes as input future projections of hourly time-series of meteorology and uses the temporal variations in weather to predict hourly profiles of total electricity demand for every county in the lower 48 states. The core predicitons in `tell` are based on a series of multilayer perceptron (MLP) models for individual Balancing Authorities (BAs). Those MLP models are trained on historical observations of weather and electricity demand. Hourly predictions from `tell` are scaled to match the annual state-level total electricity loads predicted by the U.S. version of the Global Change Analysis Model (GCAM-USA). GCAM-USA captures the long-term co-evolution of the human-Earth system. Using this unique approach allows `tell` to reflect both changes in the shape of the load profile due to variations in weather and the long-term evolution of energy demand due to changes in population, technology, and economics. `tell` is unique from other probabilistic load forecasting models in that it features an explicit spatial component that allows us to relate predicted loads to where they would occur spatially within a grid operations model. The output of `tell` is a series of hourly predictions for future electricity demand at the county, state, and BA scale that are quantitatively and conceptually consistent with one another.\n",
    "\n",
    "## Lets get started! \n",
    "\n",
    "In this quickstarter we will walk through a series of steps for exploring `tell`, starting with importing the package and ending with visualizing the output. This quickstarter is based on a subset of forcing data for `tell`. This allows the user to walk through the entire `tell` package in a matter of minutes. More information about how the model works and how it can be fully applied are available on the [Read the Docs](https://immm-sfa.github.io/tell/index.html) site for `tell`.\n",
    "\n",
    "## 1. Install `tell`\n",
    "\n",
    "`tell` is available via GitHub repository by using the pip install functionality below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/IMMM-SFA/tell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db683a-70a2-4f89-a1d6-4c6d5f180272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the TELL package and information about your operating system:\n",
    "import os \n",
    "import tell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fce96-4576-4a2d-9db6-4d0be72b3c1d",
   "metadata": {},
   "source": [
    "## 2. Install the package of raw data underpinning TELL\n",
    "\n",
    "`tell` is based on open-source publicly accessible data. For convienence, we've packaged all of the core data underpinning `tell` into a [Zenodo data package](https://zenodo.org/record/5714756#.YhkTjxPMJTY). In order to run this notebook, first set the local directory where you would like to store the package data and the run the 'install_package_data' function below. Note that the raw data package will require ~1.6 GB of storage and can take several minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc0f66-ba83-47d4-b161-0b1ad076064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the current working directory and the \"tell_data\" directory where the data will be stored:\n",
    "current_dir =  os.path.join(os.path.dirname(os.getcwd()))\n",
    "tell_data_dir = os.path.join(current_dir, r'tell_data')\n",
    "\n",
    "# If the \"tell_data\" subdirectory doesn't exist then create it:\n",
    "if not os.path.exists(tell_data_dir):\n",
    "   os.makedirs(tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f47ed7-b16e-4bb7-9393-12b9821768da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the TELL raw data package from Zenodo:\n",
    "tell.install_tell_raw_data(data_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b10c8-5872-42a9-84ae-ba09ef4a90c2",
   "metadata": {},
   "source": [
    "## 3. Pre-process the data needed for TELL\n",
    "\n",
    "In the next few code blocks we will load and process the necessary data for `tell`. The raw data (i.e., historical loads, population, meteorology, etc.) is in a variety of different formats determined by their originating organization (e.g., the Energy Information Agency). This series of processing steps cleans the raw data and converts it into a set of simplified .csv files which can be combined together and used as input to the MLP model training steps. Follow the sequence below to pre-process the `tell` input data. If you have already finished this step you can proceed to section 4.\n",
    "\n",
    "### 3.1. Spatially mapping Balancing Authorities (BAs) to counties\n",
    "\n",
    "The code block underpins the spatial component of `tell`. The function 'map_ba_service_territory' uses information about Balancing Authorities (BAs) from the [EIA-861](https://www.eia.gov/electricity/data/eia861/) dataset to spatially map BAs to U.S. counties. This allows us to assign loads predicted for individual BAs in `tell` to where they would occur with respect to each other spatially. More information about how BAs are mapped is available on the [Read the Docs](https://immm-sfa.github.io/tell/index.html) site for `tell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab18f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute the mapping function for all years from 2015 to 2019:\n",
    "tell.map_ba_service_territory(start_year = 2015,\n",
    "                              end_year = 2019,\n",
    "                              data_input_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0cc91-fcc8-40a6-b88a-def74f59af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the output of the mapping step by plotting the service territory for a given BA (e.g., PJM, CISO, ERCO, etc.) in a given year:\n",
    "tell.plot_ba_service_territory(ba_to_plot = 'PJM',\n",
    "                               year_to_plot = '2019',\n",
    "                               data_input_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8412d0",
   "metadata": {},
   "source": [
    "### 3.2. Process the EIA-930 historical hourly demand dataset\n",
    "\n",
    "Here we convert the raw EIA-930 hourly electricity demand data for BAs from Excel files to .csv files. The function 'process_eia_930' also subsets the EIA-930 dataset to only include the desired variables. Note that this step can take a few minutes even when using parallel processing streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f6616-3de4-4a57-a3d9-75bf5adbe6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to pre-proceess the raw EIA-930 data using parallel processing streams:\n",
    "tell.process_eia_930_data(data_input_dir = tell_data_dir,\n",
    "                          n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274fd1f-de15-4987-b9fe-c30b2af57c4e",
   "metadata": {},
   "source": [
    "### 3.3. Process the historical population dataset\n",
    "\n",
    "This pre-processing step takes historical county-level population data from the U.S. Census Bureau and computes the total population living within the service territory of each BA. The historical annual populations are then interpolated to an hourly resolution in preparation to use population as input to the MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48a208-9fcc-4325-ab1b-cfce623bab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to pre-process the historical observed population data for all years from 2015 to 2019:\n",
    "tell.process_ba_population_data(start_year = 2015,\n",
    "                                end_year = 2019,\n",
    "                                data_input_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a3913-25ec-4d9f-8544-ecff68e4d467",
   "metadata": {},
   "source": [
    "### 3.4. Process the sample historical and future meteorology data\n",
    "\n",
    "This pre-processing step relies on another pre-packaged sample dataset containing historical and sample future weather data used in `tell`. This sample weather data is also available from a [Zenodo data package](https://zenodo.org/record/6326507#.YiFa9hPMJTY). This section of the quickstarter downloads that sample data. The historical and future meteorology is based on IM3 simulations using the Weather Research and Forecasting (WRF) model. Meteorological output from WRF is first averaged into county-level mean values and then population-weighted to create an hourly time series of meteorology for each BA in `tell`.  Note that the sample weather data package will require ~250 MB of storage and can take several minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba953ea1-8e71-4a99-86ba-81b27e4534d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the TELL sample weather data package from Zenodo:\n",
    "tell.install_sample_weather_data(data_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f5df8-16d9-400c-acdf-2f0d34b971f2",
   "metadata": {},
   "source": [
    "### 3.5 Compile the historical hourly load, population, and meteorology data \n",
    "\n",
    "In this final pre-processing step we compile the historical load, population, and meteorology data into a single set of .csv files. There is one .csv file generated for each BA in `tell`. These composite .csv files are then used as input to the MLP model training step that starts in the next section. Note that this step can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6d6f4-4889-4c76-9375-9f37d365dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the hourly load data, population data, and wrf climate data by date\n",
    "tell.compile_data(start_year = 2015,\n",
    "                  end_year = 2019,\n",
    "                  data_input_dir = tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a037554",
   "metadata": {},
   "source": [
    "## 4. Multilayer Perceptron (MLP) model training and prediction\n",
    "\n",
    "This step takes the data processed and compiled above and runs a multilayer perceptron (MLP) model to predict future hourly load. Start-time is the start-time for analysis, end-time is the end time for analysis and spilt-time is the timestamp splitting train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for the mlp output\n",
    "mlp_output_dir =  os.path.join(current_dir, r'outputs', r'mlp_output')\n",
    "\n",
    "if not os.path.exists(mlp_output_dir):\n",
    "   os.makedirs(mlp_output_dir)\n",
    "\n",
    "# specify the parameters of the MLP model\n",
    "batch_run = True\n",
    "target_ba_list = ('AECI', 'SWPP', 'TAL')\n",
    "generate_plots = True\n",
    "start_time = \"2019-01-01 00:00:00\"\n",
    "end_time = \"2019-12-31 23:00:00\"\n",
    "split_time = \"2019-06-01 00:00:00\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac69fd-6962-4302-ab7f-256d9ad733eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tell.predict(compile_output_dir ,\n",
    "            mlp_output_dir,\n",
    "            start_time = start_time,\n",
    "            end_time = end_time,\n",
    "            split_time = split_time,\n",
    "            batch_run = batch_run,\n",
    "            target_ba_list = target_ba_list,\n",
    "            generate_plots = generate_plots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9048639",
   "metadata": {},
   "source": [
    "## 5. Model forward execution\n",
    "\n",
    "This script takes the .csv files produced by the TELL MLP model and distributes the predicted load to the counties that each balancing authority (BA) operates in. The county-level hourly loads are then summed to the state-level and scaled to match the state-level annual loads produced by GCAM-USA. Three sets of output files are generated: county-level hourly loads, state-level hourly loads, and hourly loads for each BA. There is one additional summary output file that includes state-level annual loads from TELL and GCAM-USA as well as the scaling factors.\n",
    "\n",
    "Please set the directories below to your local machine preferences and run the tell.execute_forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the year and GCAM-USA scenario to process:\n",
    "year_to_process = '2020'\n",
    "gcam_usa_scenario = 'scenario_name'\n",
    "\n",
    "# Set the data input and output directories:\n",
    "mlp_input_dir = os.path.join(current_dir, 'outputs', 'mlp_output')\n",
    "ba_geolocation_input_dir = os.path.join(current_dir, 'outputs', 'fips_mapping_files')\n",
    "gcam_usa_input_dir = os.path.join(current_dir, 'raw_data')\n",
    "data_output_dir = os.path.join(current_dir, 'outputs', 'forward_output', year_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443cd7d-7ab6-4c54-869c-cff129597983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the MLP model forward in time and\n",
    "tell.execute_forward(year_to_process, \n",
    "                     mlp_input_dir, \n",
    "                     ba_geolocation_input_dir,\n",
    "                     pop_input_dir, \n",
    "                     gcam_usa_input_dir, \n",
    "                     data_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae90672",
   "metadata": {},
   "source": [
    "## 6. Model visualization\n",
    "\n",
    "Below are a few select model visualizations to check on model performance for select states and BAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data input and output directories:\n",
    "data_input_dir = os.path.join(current_dir, 'outputs', 'forward_output')\n",
    "image_output_dir = os.path.join(current_dir, r'outputs', r'image_output')\n",
    "shapefile_input_dir = raw_data_dir\n",
    "\n",
    "# Set the year of TELL output to visualize:\n",
    "year_to_plot = '2020'\n",
    "\n",
    "# Choose whether or not to save the images and set the image resolution:\n",
    "save_images = 0 # (1 = Yes)\n",
    "image_resolution = 150 # (dpi)\n",
    "\n",
    "sample_state = 'California'\n",
    "sample_ba = 'ERCO'\n",
    "\n",
    "# If you want to save the images, check that the image output directory exist and if not then create it:\n",
    "if save_images == 0:\n",
    "   if os.path.exists((image_output_dir + year_to_plot)) == False:\n",
    "      os.mkdir((image_output_dir + year_to_plot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc207feb",
   "metadata": {},
   "source": [
    "### 6.1 Plot a map of the state scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell.plot_state_scaling_factors(shapefile_input_dir, \n",
    "                                data_input_dir, \n",
    "                                year_to_plot, \n",
    "                                save_images, \n",
    "                                image_resolution,\n",
    "                                image_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e293f",
   "metadata": {},
   "source": [
    "### 6.2 Plot the state annual total loads from GCAM-USA and TELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell.plot_state_annual_total_loads(state_summary_df, \n",
    "                                   year_to_plot, \n",
    "                                   save_images, \n",
    "                                   image_resolution, \n",
    "                                   image_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c21d43",
   "metadata": {},
   "source": [
    "### 6.3 Plot the time-series of total hourly loads for a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell.plot_state_load_time_series({state}, \n",
    "                                 state_hourly_load_df, \n",
    "                                 year_to_plot, \n",
    "                                 save_images, \n",
    "                                 image_resolution,\n",
    "                                 image_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18520f3",
   "metadata": {},
   "source": [
    "### 6.4 Plot the load duration curve for a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell.plot_state_load_duration_curve({state}, \n",
    "                                    state_hourly_load_df, \n",
    "                                    year_to_plot, \n",
    "                                    save_images, \n",
    "                                    image_resolution,\n",
    "                                    image_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2702",
   "metadata": {},
   "source": [
    "### 6.5 Plot the time-series of total hourly loads for a given BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5c303-a15e-4c9c-b97a-462f30618328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell.plot_ba_load_time_series({sample_ba}, \n",
    "                              ba_hourly_load_df, \n",
    "                              year_to_plot, \n",
    "                              save_images, \n",
    "                              image_resolution, \n",
    "                              image_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
