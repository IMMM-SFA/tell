{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e071b84",
   "metadata": {},
   "source": [
    "# TELL Runs for IM3's Experiment Group B \n",
    "\n",
    "This notebook executes the initial set of runs of the TELL model for IM3's Experiment Group B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db683a-70a2-4f89-a1d6-4c6d5f180272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the TELL package and information about your operating system:\n",
    "import os \n",
    "import tell\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tell.package_data import get_ba_abbreviations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fce96-4576-4a2d-9db6-4d0be72b3c1d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc0f66-ba83-47d4-b161-0b1ad076064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top-level directory and the subdirectory where the data will be stored:\n",
    "current_dir =  '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs'\n",
    "tell_data_dir = os.path.join(current_dir, r'tell_data')\n",
    "\n",
    "# If the \"tell_data_dir\" subdirectory doesn't exist then create it:\n",
    "if not os.path.exists(tell_data_dir):\n",
    "   os.makedirs(tell_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a037554",
   "metadata": {},
   "source": [
    "## Run the MLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6b851-29c9-4a1b-a149-ee341f1af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of BA abbreviations to process:\n",
    "ba_abbrev_list = tell.get_balancing_authority_to_model_dict().keys()\n",
    "\n",
    "scenario_to_process = 'rcp85hotter_ssp5'\n",
    "\n",
    "# Run the MLP prediction step for the list of BAs using parallel processing streams:\n",
    "for year_to_process in range(2020,2100,1):\n",
    "    pdf = tell.predict_batch(target_region_list = ba_abbrev_list,\n",
    "                             year = year_to_process,\n",
    "                             data_dir = os.path.join(tell_data_dir, r'wrf_to_tell_data', scenario_to_process),\n",
    "                             datetime_field_name = 'Time_UTC',\n",
    "                             save_prediction = True,\n",
    "                             prediction_output_directory = os.path.join(tell_data_dir, r'outputs', r'mlp_output', scenario_to_process),\n",
    "                             n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9048639",
   "metadata": {},
   "source": [
    "## Run the Forward Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the TELL model forward in time for a given year in five year increments out to 2100:\n",
    "for year in range(2090,2100,1):\n",
    "    summary_df, ba_time_series_df, state_time_series_df = tell.execute_forward(year_to_process = str(year),\n",
    "                                                                               gcam_target_year = '2095', \n",
    "                                                                               scenario_to_process = 'rcp85hotter_ssp5',\n",
    "                                                                               data_output_dir = '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs/tell_data/outputs/tell_output',\n",
    "                                                                               gcam_usa_input_dir = '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs/tell_data/gcamusa_data',\n",
    "                                                                               map_input_dir = '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs/tell_data/ba_service_territory_data',\n",
    "                                                                               mlp_input_dir = '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs/tell_data/outputs/mlp_output',\n",
    "                                                                               pop_input_dir = '/Users/burl878/Documents/Research/IMMM/Data/TELL/Production_Runs/tell_data/population_data',\n",
    "                                                                               save_county_data = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47db5ee-9da6-4368-82fb-75820c55fe28",
   "metadata": {},
   "source": [
    "## Visualize the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc3caf-cb82-4d99-898c-e735013efb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the interannual variability for a given BA or state:\n",
    "def plot_load_duration_curve_variability(plot_ba, entity_to_plot: str, top_x_hours: int, scenario_to_plot: str, data_input_dir: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \"\"\"Plot the interannual variability of the load duration curve for a given BA or state\n",
    "\n",
    "    :param plot_ba:             Set to True if you want to plot the output for BAs\n",
    "    :type plot_ba:              bool\n",
    "\n",
    "    :param entity_to_plot:      BA code or state abbreviation for entity you want to plot\n",
    "    :type entity_to_plot:       str\n",
    "\n",
    "    :param top_x_hours:         Number of peak hours to be included in the load duration curve plot\n",
    "    :type top_x_hours:          int\n",
    "\n",
    "    :param scenario_to_plot:    Scenario you want to plot\n",
    "    :type scenario_to_plot:     str\n",
    "\n",
    "    :param data_input_dir:      Top-level data directory for TELL\n",
    "    :type data_input_dir:       str\n",
    "\n",
    "    :param image_output_dir:    Directory to store the images\n",
    "    :type image_output_dir:     str\n",
    "\n",
    "    :param image_resolution:    Resolution at which you want to save the images in DPI\n",
    "    :type image_resolution:     int\n",
    "\n",
    "    :param save_images:         Set to True if you want to save the images after they're generated\n",
    "    :type save_images:          bool\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the data input directory:\n",
    "    tell_data_input_dir = os.path.join(data_input_dir, r'outputs', r'tell_output', scenario_to_plot)\n",
    "\n",
    "    # Set the image output directory:\n",
    "    image_dir = os.path.join(image_output_dir, scenario_to_plot)\n",
    "\n",
    "    # If the output directory doesn't exist then create it:\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(25, 10))\n",
    "        \n",
    "    for base_year in range(2025,2105,10):\n",
    "        if plot_ba:\n",
    "           for year_delta in range(-5,5,1):\n",
    "               # Read in the 'TELL_Balancing_Authority_Hourly_Load_Data_' .csv file and parse the time variable:\n",
    "               hourly_load_df = pd.read_csv((tell_data_input_dir + '/' + str(base_year) + '/' + 'TELL_Balancing_Authority_Hourly_Load_Data_' + str(base_year + year_delta) + '_Scaled_' + str(base_year) + '.csv'), parse_dates=[\"Time_UTC\"])\n",
    "\n",
    "               # Subset the dataframe to only the BA you want to plot:\n",
    "               subset_df = hourly_load_df.loc[hourly_load_df['BA_Code'] == entity_to_plot]\n",
    "\n",
    "               # Sort the hourly load values from largest to smallest and compute the hourly duration for each value:\n",
    "               load_df_sorted_df = subset_df.sort_values(by=['Scaled_TELL_BA_Load_MWh'], ascending=False)\n",
    "               load_df_sorted_df['Year'] = str(base_year + year_delta)\n",
    "               load_df_sorted_df['Interval'] = 1\n",
    "               load_df_sorted_df['Duration'] = load_df_sorted_df['Interval'].cumsum()\n",
    "                  \n",
    "               # Rename the load variable:\n",
    "               load_df_sorted_df.rename(columns={'Scaled_TELL_BA_Load_MWh': 'Load_MWh'}, inplace=True)\n",
    "            \n",
    "               # Subset and reorder the columns:\n",
    "               output_df = load_df_sorted_df[['Year', 'Duration', 'Load_MWh']]\n",
    "        \n",
    "               # Aggregate the output into a new dataframe:\n",
    "               if year_delta == -5:\n",
    "                  aggregate_output_df = output_df\n",
    "               else:\n",
    "                  aggregate_output_df = pd.concat([aggregate_output_df, output_df])\n",
    "\n",
    "           aggregate_output_df = aggregate_output_df.loc[aggregate_output_df['Duration'] <= top_x_hours]\n",
    "        \n",
    "           if base_year == 2025:\n",
    "              index = 1\n",
    "           else:\n",
    "              index = int(((base_year - 2025)/10) + 1)\n",
    "        \n",
    "           plt.subplot(2,4,index)\n",
    "           for year_delta in range(-5,6,1):\n",
    "               plt.plot(aggregate_output_df['Duration'].loc[aggregate_output_df['Year'] == str(base_year + year_delta)], \n",
    "                        aggregate_output_df['Load_MWh'].loc[aggregate_output_df['Year'] == str(base_year + year_delta)], \n",
    "                        color='gray', linestyle='-', label=('Load: ' + str(base_year + year_delta)), linewidth=1)\n",
    "           plt.plot(aggregate_output_df['Duration'].loc[aggregate_output_df['Year'] == str(base_year)], aggregate_output_df['Load_MWh'].loc[aggregate_output_df['Year'] == str(base_year)],\n",
    "                    color='black', linestyle='-', label='Base Year Load', linewidth=4)\n",
    "           plt.xlim((0,top_x_hours))\n",
    "           plt.xlabel(\"Duration [h]\")\n",
    "           plt.ylabel(\"Hourly Load [MWh]\")\n",
    "           plt.title((entity_to_plot + ' LDCs in ' + str(base_year) + ': ' + scenario_to_plot))\n",
    "        \n",
    "        if plot_ba == False:\n",
    "           for year_delta in range(-5,5,1):\n",
    "               # Read in the 'TELL_State_Summary_Data' .csv file and parse the time variable:\n",
    "               hourly_load_df = pd.read_csv((tell_data_input_dir + '/' + str(base_year) + '/' + 'TELL_State_Hourly_Load_Data_' + str(base_year + year_delta) + '_Scaled_' + str(base_year) + '.csv'), parse_dates=[\"Time_UTC\"])\n",
    "\n",
    "               # Subset the dataframe to only the state you want to plot:\n",
    "               subset_df = hourly_load_df.loc[hourly_load_df['State_Name'] == entity_to_plot]\n",
    "\n",
    "               # Sort the hourly load values from largest to smallest and compute the hourly duration for each value:\n",
    "               load_df_sorted_df = subset_df.sort_values(by=['Scaled_TELL_State_Load_MWh'], ascending=False)\n",
    "               load_df_sorted_df['Year'] = str(base_year + year_delta)\n",
    "               load_df_sorted_df['Interval'] = 1\n",
    "               load_df_sorted_df['Duration'] = load_df_sorted_df['Interval'].cumsum()\n",
    "                  \n",
    "               # Rename the load variable:\n",
    "               load_df_sorted_df.rename(columns={'Scaled_TELL_State_Load_MWh': 'Load_MWh'}, inplace=True)\n",
    "            \n",
    "               # Subset and reorder the columns:\n",
    "               output_df = load_df_sorted_df[['Year', 'Duration', 'Load_MWh']]\n",
    "        \n",
    "               # Aggregate the output into a new dataframe:\n",
    "               if year_delta == -5:\n",
    "                  aggregate_output_df = output_df\n",
    "               else:\n",
    "                  aggregate_output_df = pd.concat([aggregate_output_df, output_df])\n",
    "\n",
    "           aggregate_output_df = aggregate_output_df.loc[aggregate_output_df['Duration'] <= top_x_hours]\n",
    "        \n",
    "           if base_year == 2025:\n",
    "              index = 1\n",
    "           else:\n",
    "              index = int(((base_year - 2025)/10) + 1)\n",
    "        \n",
    "           plt.subplot(2,4,index)\n",
    "           for year_delta in range(-5,6,1):\n",
    "               plt.plot(aggregate_output_df['Duration'].loc[aggregate_output_df['Year'] == str(base_year + year_delta)], \n",
    "                        aggregate_output_df['Load_MWh'].loc[aggregate_output_df['Year'] == str(base_year + year_delta)], \n",
    "                        color='gray', linestyle='-', label=('Load: ' + str(base_year + year_delta)), linewidth=1)\n",
    "           plt.plot(aggregate_output_df['Duration'].loc[aggregate_output_df['Year'] == str(base_year)], aggregate_output_df['Load_MWh'].loc[aggregate_output_df['Year'] == str(base_year)],\n",
    "                    color='black', linestyle='-', label='Base Year Load', linewidth=4)\n",
    "           plt.xlim((0,top_x_hours))\n",
    "           plt.xlabel(\"Duration [h]\")\n",
    "           plt.ylabel(\"Hourly Load [MWh]\")\n",
    "           plt.title((entity_to_plot + ' LDCs in ' + str(base_year) + ': ' + scenario_to_plot))\n",
    "\n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       if plot_ba:\n",
    "          ba_name = entity_to_plot\n",
    "          filename = ('TELL_BA_LDC_Variability_' + ba_name + '_' + scenario_to_plot + '.png')\n",
    "          plt.savefig(os.path.join(image_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')   \n",
    "       if plot_ba == False:\n",
    "          state_name = entity_to_plot\n",
    "          state_name = state_name.replace(\" \", \"_\")\n",
    "          filename = ('TELL_State_LDC_Variability_' + state_name + '_' + scenario_to_plot + '.png')\n",
    "          plt.savefig(os.path.join(image_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175a3e4-9ffb-4439-bcdd-42c6220f50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of BAs to process:\n",
    "#ba_names = get_ba_abbreviations()\n",
    "ba_names = ['CISO', 'PJM', 'ERCO', 'ISNE', 'MISO', 'AZPS']\n",
    "\n",
    "# Loop over the list of BAs and make the LDC plot for each BA:\n",
    "for i in ba_names:\n",
    "    plot_load_duration_curve_variability(plot_ba = True,\n",
    "                                         entity_to_plot = i, \n",
    "                                         top_x_hours = 168, \n",
    "                                         scenario_to_plot = 'rcp85hotter_ssp5', \n",
    "                                         data_input_dir = tell_data_dir,\n",
    "                                         image_output_dir = '/Users/burl878/Documents/code_repos/tell/tell/production_visualizations', \n",
    "                                         image_resolution = 125, \n",
    "                                         save_images = True)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aac3bf-6f36-4cf0-92ba-043c0f399445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of states to process:\n",
    "state_names = ['California', 'Texas', 'New York', 'Pennsylvania', 'Florida', 'Washington']\n",
    "\n",
    "# Loop over the list of states and make the LDC plot for each state:\n",
    "for i in state_names:\n",
    "    plot_load_duration_curve_variability(plot_ba = False,\n",
    "                                         entity_to_plot = i, \n",
    "                                         top_x_hours = 168, \n",
    "                                         scenario_to_plot = 'rcp85hotter_ssp5', \n",
    "                                         data_input_dir = tell_data_dir,\n",
    "                                         image_output_dir = '/Users/burl878/Documents/code_repos/tell/tell/production_visualizations', \n",
    "                                         image_resolution = 125, \n",
    "                                         save_images = True)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc0781-a4e2-45c3-bb76-58134a6281e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the mean deviation for peak loads across BAs or states:\n",
    "def process_peak_load_variability(process_bas,  top_x_hours: int, scenario_to_process: str, data_input_dir: str):\n",
    "    \"\"\"Process the interannual variability of peak loads across BAs or states:\n",
    "\n",
    "    :param process_bas:         Set to True if you want to process the output for BAs\n",
    "    :type process_bas:          bool\n",
    "\n",
    "    :param top_x_hours:         Number of peak hours to be included in the analysis\n",
    "    :type top_x_hours:          int\n",
    "\n",
    "    :param scenario_to_process: Scenario you want to process\n",
    "    :type scenario_to_process:  str\n",
    "\n",
    "    :param data_input_dir:      Path to where the load output lives\n",
    "    :type data_input_dir:       str\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the data input directory:\n",
    "    tell_data_input_dir = os.path.join(data_input_dir, r'outputs', r'tell_output', scenario_to_process)\n",
    "\n",
    "    # Set the output directory:\n",
    "    output_dir = os.path.join(data_input_dir, r'outputs', r'postprocessed', scenario_to_process)\n",
    "\n",
    "    # If the output directory doesn't exist then create it:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Initiate an empty dataframe and counter to store the results:\n",
    "    stats_df = pd.DataFrame()\n",
    "    counter = -1;\n",
    "    \n",
    "    for base_year in range(2025,2100,10):\n",
    "        if process_bas:\n",
    "           for year_delta in range(-5,5,1):\n",
    "               # Read in the 'TELL_Balancing_Authority_Hourly_Load_Data_' .csv file and parse the time variable:\n",
    "               hourly_load_df = pd.read_csv((tell_data_input_dir + '/' + str(base_year) + '/' + 'TELL_Balancing_Authority_Hourly_Load_Data_' + str(base_year + year_delta) + '_Scaled_' + str(base_year) + '.csv'), parse_dates=[\"Time_UTC\"])\n",
    "\n",
    "               # Rename the load variable:\n",
    "               hourly_load_df.rename(columns={'Scaled_TELL_BA_Load_MWh': 'Load_MWh'}, inplace=True)         \n",
    "          \n",
    "               # Make a list of all of the unique BAs in \"hourly_load_df\":\n",
    "               unique_bas = hourly_load_df['BA_Code'].unique()\n",
    "            \n",
    "               # Loop over the BAs and process their top loads in the base year:\n",
    "               for i in range(len(unique_bas)):\n",
    "                   # Subset to just the data for the BA being processed:\n",
    "                   subset_df = hourly_load_df[hourly_load_df['BA_Code'].isin([unique_bas[i]])].copy()\n",
    "        \n",
    "                   # Sort the hourly load values from largest to smallest and compute the hourly duration for each value:\n",
    "                   load_df_sorted_df = subset_df.sort_values(by=['Load_MWh'], ascending=False)\n",
    "                   load_df_sorted_df['Year'] = str(base_year + year_delta)\n",
    "                   load_df_sorted_df['Target_Year'] = str(base_year)\n",
    "                   load_df_sorted_df['Interval'] = 1\n",
    "                   load_df_sorted_df['Duration'] = load_df_sorted_df['Interval'].cumsum()\n",
    "               \n",
    "                   # Subset to the top X hours:\n",
    "                   load_df_sorted_df = load_df_sorted_df.loc[(load_df_sorted_df['Duration'] <= top_x_hours)]\n",
    "                \n",
    "                   # Subset and reorder the columns:\n",
    "                   output_df = load_df_sorted_df[['BA_Code', 'Year', 'Target_Year', 'Duration', 'Load_MWh']]\n",
    "                \n",
    "                   # Aggregate the output into a new dataframe:\n",
    "                   if (i == 0) and (year_delta == -5):\n",
    "                      ba_output_df = output_df\n",
    "                   else:\n",
    "                      ba_output_df = pd.concat([ba_output_df, output_df])\n",
    "          \n",
    "           # Subset the dataframe the base year and the out years:\n",
    "           base_year_df = ba_output_df.loc[ba_output_df['Year'] == str(base_year)].copy()\n",
    "           out_year_df = ba_output_df.loc[ba_output_df['Year'] != str(base_year)].copy()\n",
    "            \n",
    "           # Rename the load variable in the base year dataframe:\n",
    "           base_year_df.rename(columns={'Load_MWh': 'Base_Year_Load_MWh'}, inplace=True)\n",
    "           \n",
    "           # Merge the out year and base year dataframes together based on common BA_Code and Duration variables:\n",
    "           merged_df = pd.merge(out_year_df, base_year_df, how='left', on=['BA_Code', 'Duration'])\n",
    "        \n",
    "           # Subset and reorder the columns:\n",
    "           merged_df = merged_df[['BA_Code', 'Duration', 'Load_MWh', 'Base_Year_Load_MWh']]\n",
    "            \n",
    "           # Calculate the absolute and relative load difference:\n",
    "           merged_df['Absolute_Load_Difference_MWh'] = abs(merged_df['Base_Year_Load_MWh'] - merged_df['Load_MWh']) \n",
    "           merged_df['Relative_Load_Difference_%'] = (merged_df['Absolute_Load_Difference_MWh'] / merged_df['Base_Year_Load_MWh']) * 100 \n",
    "            \n",
    "           # Make a list of all of the unique BAs in \"hourly_load_df\":\n",
    "           unique_bas = merged_df['BA_Code'].unique()\n",
    "            \n",
    "           # Loop over the BAs and compute their mean relative bias:\n",
    "           for i in range(len(unique_bas)):\n",
    "               # Advance the counter by one:\n",
    "               counter = counter + 1\n",
    "                \n",
    "               # Subset to just the data for the BA being processed:\n",
    "               subset_df = merged_df[merged_df['BA_Code'].isin([unique_bas[i]])].copy()\n",
    "               \n",
    "               stats_df.loc[counter, 'Year'] = str(base_year)\n",
    "               stats_df.loc[counter, 'BA_Code'] = unique_bas[i]\n",
    "               stats_df.loc[counter, 'Absolute_Load_Difference_MWh'] = subset_df['Absolute_Load_Difference_MWh'].mean().round(2)\n",
    "               stats_df.loc[counter, 'Relative_Load_Difference_%'] = subset_df['Relative_Load_Difference_%'].mean().round(2)\n",
    "    \n",
    "        if process_bas == False:\n",
    "           for year_delta in range(-5,5,1):\n",
    "               # Read in the 'TELL_State_Summary_Data' .csv file and parse the time variable:\n",
    "               hourly_load_df = pd.read_csv((tell_data_input_dir + '/' + str(base_year) + '/' + 'TELL_State_Hourly_Load_Data_' + str(base_year + year_delta) + '_Scaled_' + str(base_year) + '.csv'), parse_dates=[\"Time_UTC\"])\n",
    "              \n",
    "               # Rename the load variable:\n",
    "               hourly_load_df.rename(columns={'Scaled_TELL_State_Load_MWh': 'Load_MWh'}, inplace=True)         \n",
    "          \n",
    "               # Make a list of all of the unique states in \"hourly_load_df\":\n",
    "               unique_states = hourly_load_df['State_Name'].unique()\n",
    "            \n",
    "               # Loop over the states and process their top loads in the base year:\n",
    "               for i in range(len(unique_states)):\n",
    "                   # Subset to just the data for the state being processed:\n",
    "                   subset_df = hourly_load_df[hourly_load_df['State_Name'].isin([unique_states[i]])].copy()\n",
    "        \n",
    "                   # Sort the hourly load values from largest to smallest and compute the hourly duration for each value:\n",
    "                   load_df_sorted_df = subset_df.sort_values(by=['Load_MWh'], ascending=False)\n",
    "                   load_df_sorted_df['Year'] = str(base_year + year_delta)\n",
    "                   load_df_sorted_df['Target_Year'] = str(base_year)\n",
    "                   load_df_sorted_df['Interval'] = 1\n",
    "                   load_df_sorted_df['Duration'] = load_df_sorted_df['Interval'].cumsum()\n",
    "               \n",
    "                   # Subset to the top X hours:\n",
    "                   load_df_sorted_df = load_df_sorted_df.loc[(load_df_sorted_df['Duration'] <= top_x_hours)]\n",
    "                \n",
    "                   # Subset and reorder the columns:\n",
    "                   output_df = load_df_sorted_df[['State_Name', 'Year', 'Target_Year', 'Duration', 'Load_MWh']]\n",
    "                \n",
    "                   # Aggregate the output into a new dataframe:\n",
    "                   if (i == 0) and (year_delta == -5):\n",
    "                      state_output_df = output_df\n",
    "                   else:\n",
    "                      state_output_df = pd.concat([state_output_df, output_df])\n",
    "          \n",
    "           # Subset the dataframe the base year and the out years:\n",
    "           base_year_df = state_output_df.loc[state_output_df['Year'] == str(base_year)].copy()\n",
    "           out_year_df = state_output_df.loc[state_output_df['Year'] != str(base_year)].copy()\n",
    "            \n",
    "           # Rename the load variable in the base year dataframe:\n",
    "           base_year_df.rename(columns={'Load_MWh': 'Base_Year_Load_MWh'}, inplace=True)\n",
    "           \n",
    "           # Merge the out year and base year dataframes together based on common State_Name and Duration variables:\n",
    "           merged_df = pd.merge(out_year_df, base_year_df, how='left', on=['State_Name', 'Duration'])\n",
    "        \n",
    "           # Subset and reorder the columns:\n",
    "           merged_df = merged_df[['State_Name', 'Duration', 'Load_MWh', 'Base_Year_Load_MWh']]\n",
    "            \n",
    "           # Calculate the absolute and relative load difference:\n",
    "           merged_df['Absolute_Load_Difference_MWh'] = abs(merged_df['Base_Year_Load_MWh'] - merged_df['Load_MWh']) \n",
    "           merged_df['Relative_Load_Difference_%'] = (merged_df['Absolute_Load_Difference_MWh'] / merged_df['Base_Year_Load_MWh']) * 100 \n",
    "            \n",
    "           # Make a list of all of the unique states in \"hourly_load_df\":\n",
    "           unique_states = hourly_load_df['State_Name'].unique()\n",
    "            \n",
    "           # Loop over the states and compute their mean relative bias:\n",
    "           for i in range(len(unique_states)):\n",
    "               # Advance the counter by one:\n",
    "               counter = counter + 1\n",
    "                \n",
    "               # Subset to just the data for the BA being processed:\n",
    "               subset_df = merged_df[merged_df['State_Name'].isin([unique_states[i]])].copy()\n",
    "               \n",
    "               stats_df.loc[counter, 'Year'] = str(base_year)\n",
    "               stats_df.loc[counter, 'State_Name'] = unique_states[i]\n",
    "               stats_df.loc[counter, 'Absolute_Load_Difference_MWh'] = subset_df['Absolute_Load_Difference_MWh'].mean().round(2)\n",
    "               stats_df.loc[counter, 'Relative_Load_Difference_%'] = subset_df['Relative_Load_Difference_%'].mean().round(2)\n",
    "              \n",
    "    if process_bas:\n",
    "       csv_output_filename = os.path.join(output_dir + '/BA_Peak_Load_Variability_Top_' + str(top_x_hours) + '_Hours.csv')\n",
    "    if process_bas == False:\n",
    "       csv_output_filename = os.path.join(output_dir + '/State_Peak_Load_Variability_Top_' + str(top_x_hours) + '_Hours.csv')\n",
    "    \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    stats_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "    \n",
    "    return stats_df\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f228f-b338-464c-87e4-b42f90fd8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_stats_df = process_peak_load_variability(process_bas = True,\n",
    "                                            top_x_hours = 168, \n",
    "                                            scenario_to_process = 'rcp85hotter_ssp5', \n",
    "                                            data_input_dir = tell_data_dir)\n",
    "\n",
    "ba_stats_df\n",
    "\n",
    "states_stats_df = process_peak_load_variability(process_bas = False,\n",
    "                                                top_x_hours = 168, \n",
    "                                                scenario_to_process = 'rcp85hotter_ssp5', \n",
    "                                                data_input_dir = tell_data_dir)\n",
    "\n",
    "states_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e031fee-1e86-4c90-9319-c978ec868662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the mean deviation for peak loads across BAs or states:\n",
    "def plot_peak_load_variability(plot_bas, top_x_hours: int, scenario_to_plot: str, data_input_dir: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \"\"\"Plot the interannual variability of mean deviation for peak loads across BAs or states:\n",
    "\n",
    "    :param plot_bas:            Set to True if you want to plot the output for BAs\n",
    "    :type plot_bas:             bool\n",
    "\n",
    "    :param top_x_hours:         Number of peak hours to be included in the analysis\n",
    "    :type top_x_hours:          int\n",
    "\n",
    "    :param scenario_to_plot:    Scenario you want to plot\n",
    "    :type scenario_to_plot:     str\n",
    "\n",
    "    :param data_input_dir:      Top-level data directory for TELL\n",
    "    :type data_input_dir:       str\n",
    "\n",
    "    :param image_output_dir:    Directory to store the images\n",
    "    :type image_output_dir:     str\n",
    "\n",
    "    :param image_resolution:    Resolution at which you want to save the images in DPI\n",
    "    :type image_resolution:     int\n",
    "\n",
    "    :param save_images:         Set to True if you want to save the images after they're generated\n",
    "    :type save_images:          bool\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the data input directory:\n",
    "    tell_data_input_dir = os.path.join(data_input_dir, r'outputs', r'postprocessed', scenario_to_plot)\n",
    "\n",
    "    # Set the image output directory:\n",
    "    image_dir = os.path.join(image_output_dir, scenario_to_plot)\n",
    "\n",
    "    # If the output directory doesn't exist then create it:\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "    \n",
    "    # Read in the output file created by the \"process_peak_load_variability\" function:\n",
    "    if plot_bas:\n",
    "       df = pd.read_csv(tell_data_input_dir + '/BA_Peak_Load_Variability_Top_' + str(top_x_hours) + '_Hours.csv')\n",
    "    if plot_bas == False:\n",
    "       df = pd.read_csv(tell_data_input_dir + '/State_Peak_Load_Variability_Top_' + str(top_x_hours) + '_Hours.csv')\n",
    "    \n",
    "    # Make the plot:\n",
    "    fig, axes = plt.subplots(2,1,figsize=(25, 10), sharex=True, sharey=False)\n",
    "    a = df.boxplot(column=['Absolute_Load_Difference_MWh'], by=['Year'], ax=axes.flatten()[0])\n",
    "    if plot_bas:\n",
    "       a.set_title(('Load Difference Across Balancing Authorities For Top ' + str(top_x_hours) + ' Hours: ' + scenario_to_plot))\n",
    "    if plot_bas == False:\n",
    "       a.set_title(('Load Difference Across States For Top ' + str(top_x_hours) + ' Hours: ' + scenario_to_plot))\n",
    "    a.set_xlabel('Year')\n",
    "    a.set_ylabel('Mean Absolute Load Difference [MWh]')\n",
    "    \n",
    "    b = df.boxplot(column=['Relative_Load_Difference_%'], by=['Year'], ax=axes.flatten()[1])\n",
    "    if plot_bas:\n",
    "       b.set_title(('Relative Load Difference Across Balancing Authorities For Top ' + str(top_x_hours) + ' Hours: ' + scenario_to_plot))\n",
    "    if plot_bas == False:\n",
    "       b.set_title(('Relative Load Difference Across States For Top ' + str(top_x_hours) + ' Hours: ' + scenario_to_plot))\n",
    "    b.set_xlabel('Year')\n",
    "    b.set_ylabel('Mean Relative Load Difference [%]')\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "        if plot_bas:\n",
    "           filename = ('Peak_Load_Variability_BAs_Top_' + str(top_x_hours) + '_' + scenario_to_plot + '.png')\n",
    "        if plot_bas == False:\n",
    "           filename = ('Peak_Load_Variability_States_Top_' + str(top_x_hours) + '_' + scenario_to_plot + '.png')\n",
    "        plt.savefig(os.path.join(image_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802be3a7-4849-4495-b27c-56939eca0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peak_load_variability(plot_bas = True,\n",
    "                           top_x_hours = 168, \n",
    "                           scenario_to_plot = 'rcp85hotter_ssp5', \n",
    "                           data_input_dir = tell_data_dir,\n",
    "                           image_output_dir = '/Users/burl878/Documents/code_repos/tell/tell/production_visualizations', \n",
    "                           image_resolution = 125, \n",
    "                           save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71a62a-7fba-4209-9d66-083ca0073534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.4_tell",
   "language": "python",
   "name": "py3.9.4_tell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
